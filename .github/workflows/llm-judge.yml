name: LLM Judge - Deployment Decision

#evaluate detection rules based on empirical ES test results
#make deployment decision and stage approved rules for human review

on:
  workflow_run:
    workflows: ["Integration Test Detection Rules"]
    types: [completed]
    branches: [main]
  workflow_dispatch:
    inputs:
      integration_run_id:
        description: 'Integration test run ID to evaluate (optional)'
        required: false

permissions:
  contents: write
  pull-requests: write
  actions: read

jobs:
  llm-judge:
    name: Evaluate Rules for Deployment
    runs-on: ubuntu-latest
    timeout-minutes: 20

    #only run if integration tests succeeded
    if: ${{ github.event.workflow_run.conclusion == 'success' || github.event_name == 'workflow_dispatch' }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          echo "Installing dependencies..."
          pip install --upgrade pip
          pip install -r requirements.txt
          echo "✓ Dependencies installed"

      - name: Download Integration Test Results
        uses: actions/download-artifact@v4
        with:
          name: integration-test-results
          path: integration_results/
          run-id: ${{ github.event.inputs.integration_run_id || github.event.workflow_run.id }}
          github-token: ${{ secrets.GITHUB_TOKEN }}

      - name: Download Detection Rules
        uses: actions/download-artifact@v4
        with:
          name: detection-rules
          path: generated/
          run-id: ${{ github.event.inputs.integration_run_id || github.event.workflow_run.id }}
          github-token: ${{ secrets.GITHUB_TOKEN }}

      - name: Verify Downloaded Artifacts
        run: |
          echo "Checking artifacts..."

          if [ ! -f "integration_results/integration_test_results.yml" ]; then
            echo "ERROR: No integration test results found"
            exit 1
          fi

          if [ ! -d "generated/detection_rules" ]; then
            echo "ERROR: No detection rules found"
            exit 1
          fi

          RULE_COUNT=$(find generated/detection_rules -name "*.yml" | wc -l)
          echo "Found $RULE_COUNT rules to evaluate"

      - name: Authenticate to GCP
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Run LLM Judge Evaluation
        id: judge
        env:
          GOOGLE_CLOUD_PROJECT: ${{ secrets.GCP_PROJECT_ID }}
          GOOGLE_CLOUD_LOCATION: ${{ secrets.GOOGLE_CLOUD_LOCATION }}
          GOOGLE_GENAI_USE_VERTEXAI: 'true'
        run: |
          echo "Starting LLM judge evaluation..."
          echo "Model: Gemini Pro (accuracy-optimized)"
          echo "Basis: Empirical ES integration test metrics"

          python3 scripts/run_llm_judge.py \
            --integration-results integration_results/integration_test_results.yml \
            --rules-dir generated/detection_rules \
            --output llm_judge_report.yml \
            --project ${{ secrets.GCP_PROJECT_ID }} \
            --location global

      - name: Parse Judge Decision
        id: decision
        run: |
          if [ ! -f "llm_judge_report.yml" ]; then
            echo "decision=FAILED" >> $GITHUB_OUTPUT
            echo "approved_count=0" >> $GITHUB_OUTPUT
            echo "rejected_count=0" >> $GITHUB_OUTPUT
            exit 1
          fi

          python3 - <<'EOF' >> $GITHUB_OUTPUT
import yaml
with open('llm_judge_report.yml') as f:
    report = yaml.safe_load(f)
decision = report.get('deployment_decision', 'UNKNOWN')
summary = report.get('summary', {})
print(f"decision={decision}")
print(f"approved_count={summary.get('rules_approved', 0)}")
print(f"rejected_count={summary.get('rules_rejected', 0)}")
print(f"conditional_count={summary.get('rules_conditional', 0)}")
EOF

      - name: Stage Approved Rules
        id: stage
        if: steps.decision.outputs.approved_count > 0
        run: |
          echo "Staging approved rules for human review..."

          python3 scripts/stage_approved_rules.py \
            --judge-report llm_judge_report.yml \
            --rules-dir generated/detection_rules \
            --output-dir staged_rules

          #get batch info
          BATCH_ID=$(date +%Y%m%d-%H%M%S)
          echo "batch_id=$BATCH_ID" >> $GITHUB_OUTPUT

          #count staged rules
          STAGED_COUNT=$(find staged_rules -name "*.yml" -not -path "*/tests/*" 2>/dev/null | wc -l || echo 0)
          echo "staged_count=$STAGED_COUNT" >> $GITHUB_OUTPUT
          echo "Staged $STAGED_COUNT rules for review"

      - name: Upload Judge Report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: llm-judge-report
          path: |
            llm_judge_report.yml
          retention-days: 30

      - name: Create Review PR
        if: steps.decision.outputs.approved_count > 0
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          BATCH_ID="${{ steps.stage.outputs.batch_id }}"
          APPROVED_COUNT="${{ steps.decision.outputs.approved_count }}"

          #create branch for PR
          BRANCH_NAME="detection-review-$BATCH_ID"
          git checkout -b "$BRANCH_NAME"

          #configure git
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          #commit staged rules
          git add staged_rules/
          git add llm_judge_report.yml

          git commit -m "Stage detection rules for human review

Batch: $BATCH_ID
Rules: $APPROVED_COUNT rules approved by LLM judge
Decision: ${{ steps.decision.outputs.decision }}

Quality metrics from empirical ES testing:
- Precision threshold: ≥ 0.80
- Recall threshold: ≥ 0.70
- F1 score calculated from actual alerts

Review staged_rules/ and llm_judge_report.yml for details."

          git push -u origin "$BRANCH_NAME"

          #create PR with judge report
          python3 scripts/create_review_pr.py \
            --branch "$BRANCH_NAME" \
            --batch-id "$BATCH_ID" \
            --judge-report llm_judge_report.yml

      - name: Summary
        if: success()
        run: |
          echo "================================"
          echo "✓ LLM Judge Evaluation Complete"
          echo "================================"
          echo ""
          echo "Decision: ${{ steps.decision.outputs.decision }}"
          echo "Approved: ${{ steps.decision.outputs.approved_count }}"
          echo "Rejected: ${{ steps.decision.outputs.rejected_count }}"
          echo "Conditional: ${{ steps.decision.outputs.conditional_count }}"
          echo ""
          if [ "${{ steps.decision.outputs.approved_count }}" -gt 0 ]; then
            echo "✓ Created PR for human review"
            echo "  Branch: detection-review-${{ steps.stage.outputs.batch_id }}"
            echo "  Rules staged: ${{ steps.stage.outputs.staged_count }}"
          else
            echo "No rules approved for deployment"
          fi

      - name: Report Failure
        if: failure()
        run: |
          echo "================================"
          echo "✗ LLM Judge Failed"
          echo "================================"
          echo ""
          echo "Check workflow logs for errors"
          exit 1
