name: Test Detection Rules
on:
  pull_request:
    paths:
      - 'cti_src/**'
      - 'generated/sigma_rules/**'
      - 'generated/tests/**'
      - 'scripts/**'
  push:
    branches: [main]
    paths:
      - 'cti_src/**'
      - 'generated/sigma_rules/**'
      - 'generated/tests/**'
  workflow_dispatch:

permissions:
  contents: write
  id-token: write

jobs:
  unit-test:
    name: Unit Test Sigma Rules
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install Dependencies
        run: |
          echo "Installing dependencies from requirements.txt..."
          pip install -r requirements.txt
          echo "✓ Dependencies installed successfully"

      - name: Unit Test Sigma Rules
        run: |
          python scripts/unit_test_sigma.py generated/sigma_rules/

      - name: Validate Elasticsearch Queries
        run: |
          python scripts/validate_elasticsearch_queries.py generated/sigma_rules/

      - name: Validate Test Payloads
        run: |
          python scripts/validate_test_payloads.py generated/tests/

  convert-and-validate:
    name: Convert Sigma → ELK + Validate
    runs-on: ubuntu-latest
    needs: unit-test
    steps:
      - uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install Dependencies
        run: |
          pip install -r requirements.txt

      - name: Authenticate to GCP
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Convert and Validate ELK Queries
        env:
          GOOGLE_CLOUD_PROJECT: ${{ secrets.GCP_PROJECT_ID }}
          GOOGLE_CLOUD_LOCATION: ${{ secrets.GOOGLE_CLOUD_LOCATION }}
        run: |
          echo "Converting Sigma rules to Elasticsearch queries..."
          echo "Validating with Gemini 2.5 Pro + Google Search research..."

          python scripts/convert_and_validate_elk.py \
            --rules generated/sigma_rules \
            --output generated/ELK_QUERIES.json \
            --validation-report generated/ELK_VALIDATION_REPORT.json \
            --project ${{ secrets.GCP_PROJECT_ID }} \
            --location ${{ secrets.GOOGLE_CLOUD_LOCATION }}

      - name: Check Validation Results
        id: check
        run: |
          python3 scripts/check_elk_validation.py

      - name: Upload ELK Queries
        if: success()
        uses: actions/upload-artifact@v4
        with:
          name: elk-queries
          path: |
            generated/ELK_QUERIES.json
            generated/ELK_VALIDATION_REPORT.json
          retention-days: 30

      - name: Commit ELK Queries
        if: success() && github.ref == 'refs/heads/main'
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          git add generated/ELK_QUERIES.json
          git add generated/ELK_VALIDATION_REPORT.json

          git diff --quiet && git diff --staged --quiet || git commit -m "ci: Add validated ELK queries [skip ci]"
          git push

  integration-test:
    name: Integration Test with Real Elasticsearch
    runs-on: ubuntu-latest
    needs: convert-and-validate

    steps:
      - uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install Dependencies
        run: |
          pip install -r requirements.txt

      - name: Install Elasticsearch 8.12.0
        run: |
          echo "=== Installing Elasticsearch 8.12.0 (pinned version) ==="

          #add elasticsearch GPG key and repository
          wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo gpg --dearmor -o /usr/share/keyrings/elasticsearch-keyring.gpg
          echo "deb [signed-by=/usr/share/keyrings/elasticsearch-keyring.gpg] https://artifacts.elastic.co/packages/8.x/apt stable main" | sudo tee /etc/apt/sources.list.d/elastic-8.x.list

          #update package list
          sudo apt-get update

          #install specific version (pinned for reproducibility)
          sudo apt-get install -y elasticsearch=8.12.0

          #configure for testing (disable security for local testing)
          echo "network.host: 0.0.0.0" | sudo tee -a /etc/elasticsearch/elasticsearch.yml
          echo "xpack.security.enabled: false" | sudo tee -a /etc/elasticsearch/elasticsearch.yml
          echo "discovery.type: single-node" | sudo tee -a /etc/elasticsearch/elasticsearch.yml

          #set JVM heap size (512MB for GitHub runners)
          sudo sed -i 's/-Xms1g/-Xms512m/g' /etc/elasticsearch/jvm.options
          sudo sed -i 's/-Xmx1g/-Xmx512m/g' /etc/elasticsearch/jvm.options

          #start elasticsearch
          sudo systemctl start elasticsearch

          #wait for ES to be ready (max 60s)
          echo "Waiting for Elasticsearch to start..."
          timeout 60 bash -c 'until curl -s http://localhost:9200/_cluster/health; do echo "  Still starting... ($(date +%s))"; sleep 2; done'

          #show cluster health
          echo ""
          echo "Elasticsearch cluster health:"
          curl -s http://localhost:9200/_cluster/health | jq '.'

          echo ""
          echo "Elasticsearch version:"
          curl -s http://localhost:9200 | jq '.version'

      - name: Run Integration Tests (REAL ELK - NOT MOCK)
        run: |
          echo "Running REAL integration test with Elasticsearch service"
          python scripts/integration_test_ci.py \
            --rules generated/sigma_rules \
            --tests generated/tests \
            --es-url http://localhost:9200

      - name: Show Elasticsearch Diagnostics
        if: always()
        run: |
          echo "=== Elasticsearch Diagnostics ==="

          echo ""
          echo "Indices:"
          curl -s http://localhost:9200/_cat/indices?v

          echo ""
          echo "Test logs index document count:"
          curl -s http://localhost:9200/test-logs/_count | jq '.'

          echo ""
          echo "Sample document from test-logs:"
          curl -s http://localhost:9200/test-logs/_search?size=1 | jq '.hits.hits[0]._source' || true

          echo ""
          echo "Cluster stats:"
          curl -s http://localhost:9200/_cluster/stats | jq '.indices.count, .indices.docs'

      - name: Show Elasticsearch Logs (on failure)
        if: failure()
        run: |
          echo "=== Elasticsearch Logs (last 100 lines) ==="
          sudo tail -100 /var/log/elasticsearch/elasticsearch.log || echo "No logs found"

          echo ""
          echo "=== Elasticsearch Startup Logs ==="
          sudo journalctl -u elasticsearch --no-pager | tail -50 || echo "No systemd logs"

      - name: Upload Test Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: integration-test-results
          path: generated/INTEGRATION_TEST_RESULTS.json
          retention-days: 30

      - name: Commit Test Results
        if: success() && github.ref == 'refs/heads/main'
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          # Only commit if results file exists
          if [ -f generated/INTEGRATION_TEST_RESULTS.json ]; then
            git add generated/INTEGRATION_TEST_RESULTS.json
            git diff --quiet && git diff --staged --quiet || git commit -m "ci: Update integration test results [skip ci]"
            git pull --rebase || true
            git push
          else
            echo "No integration test results to commit (tests skipped)"
          fi

      - name: Comment PR with Results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            if (!fs.existsSync('generated/INTEGRATION_TEST_RESULTS.json')) {
              console.log('No test results file found');
              return;
            }

            const results = JSON.parse(fs.readFileSync('generated/INTEGRATION_TEST_RESULTS.json', 'utf8'));

            let totalRules = Object.keys(results).length;
            let totalTP = 0, totalFP = 0, totalTN = 0, totalFN = 0;
            let perfectRules = 0;

            for (const [ruleId, metrics] of Object.entries(results)) {
              totalTP += metrics.tp;
              totalFP += metrics.fp;
              totalTN += metrics.tn;
              totalFN += metrics.fn;
              if (metrics.f1_score >= 0.90) perfectRules++;
            }

            const avgPrecision = totalTP / (totalTP + totalFP) || 0;
            const avgRecall = totalTP / (totalTP + totalFN) || 0;
            const avgF1 = totalTP + totalFP > 0 ? 2 * (avgPrecision * avgRecall) / (avgPrecision + avgRecall) : 0;

            const comment = `## Integration Test Results

            **Overall Metrics:**
            - Rules Tested: ${totalRules}
            - High-Quality Rules (F1 ≥ 0.90): ${perfectRules} (${(perfectRules/totalRules*100).toFixed(1)}%)

            **Detection Performance:**
            - True Positives: ${totalTP}
            - False Positives: ${totalFP}
            - True Negatives: ${totalTN}
            - False Negatives: ${totalFN}

            **Quality Metrics:**
            - Precision: ${avgPrecision.toFixed(2)}
            - Recall: ${avgRecall.toFixed(2)}
            - F1 Score: ${avgF1.toFixed(2)}

            ${avgF1 >= 0.75 ? '✅ **Quality PASS** - Meets deployment thresholds' : '⚠️ **Quality CONDITIONAL** - Review needed before deployment'}

            <details>
            <summary>Per-Rule Results</summary>

            | Rule | Level | TP | FP | TN | FN | Precision | Recall | F1 |
            |------|-------|----|----|----|----|-----------|--------|------|
            ${Object.entries(results).map(([id, m]) =>
              `| ${m.rule_title.substring(0, 40)} | ${m.rule_level} | ${m.tp} | ${m.fp} | ${m.tn} | ${m.fn} | ${m.precision.toFixed(2)} | ${m.recall.toFixed(2)} | ${m.f1_score.toFixed(2)} |`
            ).join('\n')}

            </details>
            `;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

  quality-gate:
    name: Quality Gate Check
    runs-on: ubuntu-latest
    needs: integration-test
    steps:
      - uses: actions/checkout@v4

      - name: Download Test Results
        uses: actions/download-artifact@v4
        with:
          name: integration-test-results
          path: generated/

      - name: Check Quality Thresholds
        run: |
          python3 << 'EOF'
          import json
          import sys

          with open('generated/INTEGRATION_TEST_RESULTS.json') as f:
              results = json.load(f)

          # Check if integration test was skipped
          if isinstance(results, dict) and results.get('status') == 'skipped':
              print("⚠️  Integration test was skipped (no test payloads)")
              print("  Rules passed unit tests and Sigma → ELK conversion")
              print("  Passing quality gate (no integration test failures)")
              sys.exit(0)

          # Normal quality metrics calculation
          total_tp = sum(r['tp'] for r in results.values())
          total_fp = sum(r['fp'] for r in results.values())
          total_tn = sum(r['tn'] for r in results.values())
          total_fn = sum(r['fn'] for r in results.values())

          precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0
          recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0
          f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

          print(f"Overall Quality Metrics:")
          print(f"  Precision: {precision:.2f}")
          print(f"  Recall: {recall:.2f}")
          print(f"  F1 Score: {f1:.2f}")
          print()

          if f1 >= 0.75:
              print("✅ PASS: Quality metrics meet deployment thresholds")
              sys.exit(0)
          elif f1 >= 0.60:
              print("⚠️  CONDITIONAL: Quality acceptable but needs review")
              sys.exit(0)
          else:
              print("❌ FAIL: Quality below acceptable thresholds")
              sys.exit(1)
          EOF
