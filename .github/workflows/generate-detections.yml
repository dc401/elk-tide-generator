name: Generate Detection Rules from CTI

#generate Elasticsearch Detection Rules and test payloads from CTI sources
#runs agent in GitHub Actions environment

on:
  workflow_dispatch:  #manual trigger
  workflow_call:  #allow reuse by other workflows (end-to-end test)
    secrets:
      GCP_SA_KEY:
        required: true
      GCP_PROJECT_ID:
        required: true
  push:
    branches: [main]
    paths:
      - 'cti_src/**'  #trigger when CTI files change
      - 'detection_agent/**'  #trigger on agent code changes
      - 'run_agent.py'  #trigger on main script changes

permissions:
  contents: write  #allow workflow to commit and push

jobs:
  generate-rules:
    name: Generate Elasticsearch Detection Rules from CTI
    runs-on: ubuntu-latest
    timeout-minutes: 60  #increased for larger CTI files with more indicators

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Clean Stale Artifacts
        run: |
          echo "Cleaning stale artifacts from previous runs..."

          #run cleanup script
          bash scripts/cleanup_staging.sh

          #clean local session_results folder (not committed but may exist from manual runs)
          rm -rf session_results/*
          echo "✓ Cleaned session_results/"

          echo "Ready for fresh generation run"

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          echo "Installing Python dependencies..."
          pip install --upgrade pip
          pip install -r requirements.txt
          echo "✓ Dependencies installed successfully"

      - name: Authenticate to GCP
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Check CTI Files
        run: |
          echo "Validating CTI files..."

          #check for valid file extensions only
          CTI_FILES=$(find cti_src -type f \( -name "*.md" -o -name "*.txt" -o -name "*.pdf" -o -name "*.docx" \))
          FILE_COUNT=$(echo "$CTI_FILES" | grep -c . || echo 0)

          echo "Found $FILE_COUNT CTI file(s)"

          #validation checks
          if [ "$FILE_COUNT" -eq 0 ]; then
            echo "ERROR: No CTI files found in cti_src/"
            exit 1
          fi

          #prevent DoS from too many files (max 50 files)
          if [ "$FILE_COUNT" -gt 50 ]; then
            echo "ERROR: Too many CTI files ($FILE_COUNT > 50)"
            echo "Process files in batches to avoid quota exhaustion"
            exit 1
          fi

          #check for suspicious file names
          SUSPICIOUS_FILES=$(find cti_src -type f -name "*../*" -o -name "*.exe" -o -name "*.sh" -o -name "*.bat")
          if [ -n "$SUSPICIOUS_FILES" ]; then
            echo "ERROR: Suspicious files detected:"
            echo "$SUSPICIOUS_FILES"
            exit 1
          fi

          #check for overly large files (>50MB per file)
          LARGE_FILES=$(find cti_src -type f -size +50M)
          if [ -n "$LARGE_FILES" ]; then
            echo "ERROR: Files exceed 50MB size limit:"
            echo "$LARGE_FILES"
            exit 1
          fi

          echo "✓ All CTI files validated"
          echo "CTI files to process:"
          echo "$CTI_FILES" | while read -r file; do
            SIZE=$(du -h "$file" | cut -f1)
            echo "  - $file ($SIZE)"
          done

      - name: Generate Detection Rules (with region retry)
        timeout-minutes: 20
        env:
          GOOGLE_CLOUD_PROJECT: ${{ secrets.GCP_PROJECT_ID }}
          GOOGLE_GENAI_USE_VERTEXAI: 'true'
        run: |
          echo "Starting Elasticsearch detection rule generation..."
          echo "Timeout: 20 minutes (with self-healing refinement + region retry)"

          #retry up to 3 times with different regions on quota exhaustion
          MAX_RETRIES=3
          for RETRY in $(seq 0 $((MAX_RETRIES-1))); do
            #select region based on retry count (shifts to next region each retry)
            SELECTED_REGION=$(python3 scripts/select_region.py $RETRY)
            echo "Attempt $((RETRY+1))/$MAX_RETRIES using region: $SELECTED_REGION"

            export GOOGLE_CLOUD_LOCATION=$SELECTED_REGION

            if python run_agent.py \
              --cti-folder cti_src \
              --output generated \
              --project ${{ secrets.GCP_PROJECT_ID }} \
              --location $SELECTED_REGION 2>&1 | tee generation.log; then
              echo "✓ Generation successful with region: $SELECTED_REGION"
              exit 0
            fi

            #check if error was quota-related (429)
            if grep -q "429.*RESOURCE_EXHAUSTED\|quota.*exhausted" generation.log; then
              echo "⚠️  Quota exhausted in $SELECTED_REGION"
              if [ $RETRY -lt $((MAX_RETRIES-1)) ]; then
                echo "   Retrying with next region..."
                sleep 5
                continue
              else
                echo "❌ All regions exhausted quota"
                exit 1
              fi
            else
              #non-quota error, fail immediately
              echo "❌ Generation failed with non-quota error"
              exit 1
            fi
          done

      - name: Verify Generated Rules
        run: |
          echo "Checking generated artifacts..."

          #count detection rules
          if [ -d "generated/detection_rules" ]; then
            RULE_COUNT=$(find generated/detection_rules -name "*.yml" | wc -l)
            echo "Generated detection rules: $RULE_COUNT"
            ls -lh generated/detection_rules/
          else
            echo "WARNING: No detection_rules directory found"
            RULE_COUNT=0
          fi

          #count test cases (embedded in rule YAML)
          if [ -f "generated/cti_context.yml" ]; then
            echo "✓ CTI context saved"
            ls -lh generated/cti_context.yml
          else
            echo "WARNING: No cti_context.yml found"
          fi

          #require at least some output
          if [ "$RULE_COUNT" -eq 0 ]; then
            echo "ERROR: No detection rules were generated"
            exit 1
          fi

      - name: Count Generated Rules
        id: count
        run: |
          #count generated rules for summary
          if [ -d "generated/detection_rules" ]; then
            RULE_COUNT=$(find generated/detection_rules -name "*.yml" 2>/dev/null | wc -l || echo 0)
            echo "rule_count=$RULE_COUNT" >> $GITHUB_OUTPUT
            echo "Generated $RULE_COUNT detection rules"
          else
            echo "rule_count=0" >> $GITHUB_OUTPUT
            echo "No rules generated"
          fi

      - name: Upload Generated Rules
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: detection-rules
          path: |
            generated/detection_rules/
            generated/cti_context.yml
          retention-days: 30

      - name: Summary
        if: success()
        run: |
          echo "================================"
          echo "✓ Detection Generation Complete"
          echo "================================"
          echo ""
          RULE_COUNT="${{ steps.count.outputs.rule_count }}"
          echo "Generated $RULE_COUNT Elasticsearch Detection Rules"
          echo ""
          echo "Generated artifacts uploaded as workflow artifacts."
          echo "Download from Actions tab to review rules."
          echo ""
          echo "Next steps:"
          echo "  1. Download artifacts and review generated rules"
          echo "  2. Run validation: scripts/validate_rules.py"
          echo "  3. Run integration tests: scripts/integration_test_ci.py"
          echo "  4. Run LLM judge: scripts/run_llm_judge.py"
