name: Integration Test Detection Rules

#test generated rules against native Elasticsearch with smart refinement
#runs after generate-detections.yml completes successfully

on:
  workflow_run:
    workflows: ["Generate Detection Rules from CTI"]
    types: [completed]
    branches: [main]
  workflow_dispatch:
    inputs:
      artifact_run_id:
        description: 'Artifact run ID from generate-detections workflow'
        required: true
        type: string

permissions:
  contents: write
  actions: read

jobs:
  integration-test:
    name: Test Rules with Native Elasticsearch
    runs-on: ubuntu-latest
    timeout-minutes: 30

    #only run if generate-detections succeeded
    if: ${{ github.event.workflow_run.conclusion == 'success' || github.event_name == 'workflow_dispatch' }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install Python dependencies
        run: |
          echo "Installing Python dependencies..."
          pip install --upgrade pip
          pip install -r requirements.txt
          echo "âœ“ Dependencies installed"

      - name: Download Generated Rules Artifact
        uses: actions/download-artifact@v4
        with:
          name: detection-rules
          path: generated/
          run-id: ${{ github.event.inputs.artifact_run_id || github.event.workflow_run.id }}
          github-token: ${{ secrets.GITHUB_TOKEN }}

      - name: Verify Downloaded Rules
        run: |
          echo "Checking downloaded artifacts..."
          if [ ! -d "generated/detection_rules" ]; then
            echo "ERROR: No detection_rules directory found in artifact"
            exit 1
          fi

          RULE_COUNT=$(find generated/detection_rules -name "*.yml" | wc -l)
          echo "Found $RULE_COUNT detection rules to test"

          if [ "$RULE_COUNT" -eq 0 ]; then
            echo "ERROR: No rules to test"
            exit 1
          fi

      - name: Authenticate to GCP
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Install Elasticsearch Dependencies
        run: |
          echo "Installing system dependencies..."
          sudo apt-get update -qq
          sudo apt-get install -y -qq curl wget gnupg apt-transport-https
          echo "âœ“ System dependencies installed"

      - name: Run Integration Tests with Refinement
        id: test
        env:
          GOOGLE_CLOUD_PROJECT: ${{ secrets.GCP_PROJECT_ID }}
          GOOGLE_CLOUD_LOCATION: ${{ secrets.GOOGLE_CLOUD_LOCATION }}
          GOOGLE_GENAI_USE_VERTEXAI: 'true'
        run: |
          echo "Starting Elasticsearch integration testing..."
          echo "Refinement: ENABLED (Gemini Pro for smart fixes)"

          python3 scripts/integration_test_ci.py \
            --rules-dir generated/detection_rules \
            --output integration_test_results.yml \
            --project ${{ secrets.GCP_PROJECT_ID }} \
            --location global

      - name: Parse Test Results
        id: results
        if: always()
        run: |
          if [ ! -f "integration_test_results.yml" ]; then
            echo "test_status=FAILED" >> $GITHUB_OUTPUT
            echo "total_rules=0" >> $GITHUB_OUTPUT
            echo "rules_passed=0" >> $GITHUB_OUTPUT
            echo "rules_failed=0" >> $GITHUB_OUTPUT
            echo "rules_refined=0" >> $GITHUB_OUTPUT
            exit 0
          fi

          #parse YAML results using python
          python3 - <<'EOF' >> $GITHUB_OUTPUT
import yaml
with open('integration_test_results.yml') as f:
    results = yaml.safe_load(f)
summary = results.get('summary', {})
print(f"total_rules={summary.get('total_rules', 0)}")
print(f"rules_passed={summary.get('rules_passed', 0)}")
print(f"rules_failed={summary.get('rules_failed', 0)}")
print(f"rules_refined={summary.get('rules_refined', 0)}")
print(f"test_status={'PASSED' if summary.get('rules_failed', 0) == 0 else 'FAILED'}")
EOF

      - name: Generate Test Report
        if: always()
        run: |
          cat > test_report.md <<'REPORT'
# Integration Test Results

**Status:** ${{ steps.results.outputs.test_status }}
**Timestamp:** $(date -u +"%Y-%m-%d %H:%M:%S UTC")

## Summary

| Metric | Value |
|--------|-------|
| Total Rules | ${{ steps.results.outputs.total_rules }} |
| âœ… Passed | ${{ steps.results.outputs.rules_passed }} |
| âŒ Failed | ${{ steps.results.outputs.rules_failed }} |
| ðŸ”§ Refined | ${{ steps.results.outputs.rules_refined }} |

## Test Environment

- **SIEM:** Native Elasticsearch (via apt)
- **Refinement:** Enabled (Gemini Pro)
- **Runtime:** ~${{ github.run_duration }} minutes
- **Workflow Run:** ${{ github.run_id }}

## Metrics Calculated

For each rule:
- **Precision:** TP / (TP + FP)
- **Recall:** TP / (TP + FN)
- **F1 Score:** 2 Ã— (Precision Ã— Recall) / (Precision + Recall)

## Refinement Process

Rules that failed thresholds (precision < 0.80 or recall < 0.70) were automatically refined by Gemini Pro based on actual ES test failures.

See `integration_test_results.yml` for detailed per-rule metrics.
REPORT

      - name: Upload Test Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: integration-test-results
          path: |
            integration_test_results.yml
            test_report.md
          retention-days: 30

      - name: Commit Refined Rules
        if: steps.results.outputs.rules_refined > 0
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          #check for changes in generated rules
          if git diff --quiet generated/detection_rules/; then
            echo "No refined rules to commit"
          else
            git add generated/detection_rules/
            git commit -m "Refine detection rules based on ES integration test failures

Auto-refined ${{ steps.results.outputs.rules_refined }} rule(s) using Gemini Pro.

Metrics improved:
- Precision threshold: â‰¥ 0.80
- Recall threshold: â‰¥ 0.70

Test run: ${{ github.run_id }}"
            git push
          fi

      - name: Summary
        if: success()
        run: |
          echo "================================"
          echo "âœ“ Integration Testing Complete"
          echo "================================"
          echo ""
          echo "Total Rules: ${{ steps.results.outputs.total_rules }}"
          echo "Passed: ${{ steps.results.outputs.rules_passed }}"
          echo "Failed: ${{ steps.results.outputs.rules_failed }}"
          echo "Refined: ${{ steps.results.outputs.rules_refined }}"
          echo ""
          echo "Next steps:"
          echo "  1. Review test results in artifacts"
          echo "  2. Run LLM judge for deployment decision"
          echo "  3. Create PR for human review"

      - name: Report Failure
        if: failure()
        run: |
          echo "================================"
          echo "âœ— Integration Testing Failed"
          echo "================================"
          echo ""
          echo "Check workflow logs for errors"
          echo ""
          if [ -f "integration_test_results.yml" ]; then
            echo "Failed rules:"
            python3 - <<'EOF'
import yaml
with open('integration_test_results.yml') as f:
    results = yaml.safe_load(f)
for rule_name, metrics in results.get('metrics', {}).items():
    if not metrics.get('pass_threshold', False):
        print(f"  - {rule_name}")
        print(f"    Precision: {metrics.get('precision', 0):.2f}")
        print(f"    Recall: {metrics.get('recall', 0):.2f}")
EOF
          fi
          exit 1
