#!/usr/bin/env python3
"""
Create GitHub PR for human review of staged detection rules
"""

import argparse
import yaml
import subprocess
from pathlib import Path
from datetime import datetime


def format_rule_table(evaluations: list) -> str:
    """format evaluations as markdown table"""
    if not evaluations:
        return "No rules"

    table = "| Rule | Quality | Decision | Precision | Recall |\n"
    table += "|------|---------|----------|-----------|--------|\n"

    for e in evaluations:
        rule_name = e['rule_name']
        quality = e['quality_score']
        decision = e['deployment_decision']
        prec = e['evaluation'].get('precision_met', False)
        rec = e['evaluation'].get('recall_met', False)

        prec_str = "âœ…" if prec else "âŒ"
        rec_str = "âœ…" if rec else "âŒ"

        table += f"| {rule_name} | {quality:.2f} | {decision} | {prec_str} | {rec_str} |\n"

    return table


def create_pr_body(batch_id: str, report: dict) -> str:
    """create formatted PR description"""

    summary = report.get('summary', {})
    evaluations = report.get('evaluations', [])
    approved = [e for e in evaluations if e['deployment_decision'] in ['APPROVE', 'CONDITIONAL']]

    body = f"""## Detection Rules for Review

**Batch ID:** {batch_id}  
**Evaluated:** {datetime.now().strftime('%Y-%m-%d %H:%M UTC')}

### Quality Summary

- **Overall Decision:** {report['deployment_decision']}
- **Average Quality Score:** {summary.get('average_quality_score', 0):.2f}/1.0
- **Rules Approved:** {summary.get('rules_approved', 0)}/{summary.get('total_rules', 0)}
- **Rules Conditional:** {summary.get('rules_conditional', 0)}
- **Rules Rejected:** {summary.get('rules_rejected', 0)}

### Rules in this PR

{format_rule_table(approved)}

### Quality Thresholds

- âœ… Precision: â‰¥ 0.80 (max 20% false positives)
- âœ… Recall: â‰¥ 0.70 (catch 70%+ of attacks)
- âœ… Overall Quality: â‰¥ 0.75

### Review Checklist

- [ ] Review detection rule YAML syntax
- [ ] Verify MITRE ATT&CK mappings are correct
- [ ] Check false positive risk assessment
- [ ] Validate test coverage (TP/FN/FP/TN)
- [ ] Confirm detection logic aligns with threat intel
- [ ] Review LLM judge recommendations

### Metadata

**Evaluation Basis:** Empirical ES integration test results  
**Judge Model:** Gemini Pro (accuracy-optimized)  
**Test Environment:** Native Elasticsearch (Ubuntu apt)

### Next Steps

1. Review staged_rules/ and llm_judge_report.yml
2. Approve this PR to move rules to production_rules/
3. Rules will be deployed via mock SIEM workflow

---

ðŸ¤– Generated by [ADK TIDE Generator](https://github.com/dc401/adk-tide-generator)
"""
    return body


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--branch', required=True, help='Branch name for PR')
    parser.add_argument('--batch-id', required=True, help='Batch ID')
    parser.add_argument('--judge-report', required=True, help='LLM judge report')

    args = parser.parse_args()

    #load judge report
    with open(args.judge_report) as f:
        report = yaml.safe_load(f)

    #create PR body
    pr_body = create_pr_body(args.batch_id, report)

    #create PR using gh CLI
    pr_title = f"Review Detection Rules - {args.batch_id}"

    cmd = [
        'gh', 'pr', 'create',
        '--base', 'main',
        '--head', args.branch,
        '--title', pr_title,
        '--body', pr_body,
        '--label', 'detection-review'
    ]

    result = subprocess.run(cmd, capture_output=True, text=True)

    if result.returncode == 0:
        print(f"âœ“ Created PR: {result.stdout.strip()}")
    else:
        print(f"âœ— Failed to create PR: {result.stderr}")
        exit(1)


if __name__ == '__main__':
    main()
